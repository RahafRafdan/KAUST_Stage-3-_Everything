{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTi4yFGluaEy"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wXbYVnSuaE4"
      },
      "source": [
        "# Pytorch Basics\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHq0eI_LuaE4"
      },
      "source": [
        "# **üìå How Data is Represented in Deep Learning?**\n",
        "\n",
        "Deep learning models process data in the form of **tensors** (multi-dimensional arrays).  \n",
        "The shape of the tensor depends on the type of data being used.\n",
        "\n",
        "## **üîπ 1Ô∏è‚É£ Tabular Data (Structured Data)**\n",
        "- **Shape:** `(batch_size, features)`\n",
        "- Each **row** is a sample, and each **column** is a feature.\n",
        "- **Handled by:** `nn.Linear` (Fully Connected Layers).\n",
        "\n",
        "## **üîπ 2Ô∏è‚É£ Image Data (Computer Vision)**\n",
        "- **Shape:** `(batch_size, channels, height, width)`\n",
        "  - **RGB Image:** `channels = 3` (Red, Green, Blue).\n",
        "  - **Grayscale Image:** `channels = 1` (sometimes omitted).\n",
        "- **Handled by:** `nn.Conv2d` (Convolutional Layers).\n",
        "\n",
        "| **Data Type** | **Tensor Shape** | **Handled by** |\n",
        "|--------------|-----------------|---------------|\n",
        "| **Tabular Data** | `(batch_size, features)` | `nn.Linear` |\n",
        "| **Image Data** | `(batch_size, channels, height, width)` | `nn.Conv2d` |\n",
        "\n",
        "‚úÖ Each data type has a specific tensor representation and requires different processing techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eCx17r3uaE5"
      },
      "source": [
        "# **üìå How to Change the Dimensions in PyTorch?**\n",
        "\n",
        "Manipulating tensor shapes is essential in deep learning. PyTorch provides several functions to modify tensor dimensions.\n",
        "\n",
        "## **üîπ 1Ô∏è‚É£ Flatten**\n",
        "- Converts **any shape** to `(batch_size, features)`.\n",
        "- **Example:**  \n",
        "  `(batch_size, channels, height, width) ‚Üí (batch_size, features)`\n",
        "\n",
        "## **üîπ 2Ô∏è‚É£ Squeeze**\n",
        "- **Removes dimensions** with size `1`.\n",
        "- **Example:**  \n",
        "  `(1, 32, 3, 28, 28) ‚Üí (32, 3, 28, 28)`\n",
        "\n",
        "## **üîπ 3Ô∏è‚É£ Unsqueeze**\n",
        "- **Adds a dimension** with size `1` at a specified position.\n",
        "- **Example:**  \n",
        "  `(3, 28, 28) ‚Üí (1, 3, 28, 28)`\n",
        "\n",
        "## **üîπ 4Ô∏è‚É£ Permute**\n",
        "- **Reorders the dimensions** of a tensor by specifying the **new order of indices**.\n",
        "- **Example:**  \n",
        "  `(32, 28, 28, 3) ‚Üí permute(0, 3, 1, 2) ‚Üí (32, 3, 28, 28)`\n",
        "\n",
        "## **üîπ 5Ô∏è‚É£ View (works similar to reshape)**\n",
        "- **Reshapes a tensor freely** while maintaining the same number of elements.\n",
        "- **Example:**  \n",
        "  `(32, 28, 28, 3) ‚Üí view(-1, 28*28*3) ‚Üí `(32, 28*28*3)`\n",
        "\n",
        "| **Operation** | **Function** | **Purpose** | **Example Transformation** |\n",
        "|--------------|-------------|-------------|----------------------------|\n",
        "| **Flatten** | `.flatten()` | Convert tensor to (batch, features) | `(32, 3, 28, 28) ‚Üí (32, 3*28*28)` |\n",
        "| **Squeeze** | `.squeeze()` | Remove dims of size 1 | `(1, 3, 28, 28) ‚Üí (3, 28, 28)` |\n",
        "| **Unsqueeze** | `.unsqueeze(dim)` | Add a dim of size 1 | `(3, 28, 28) ‚Üí (1, 3, 28, 28)` |\n",
        "| **Permute** | `.permute(dims)` | Change order of dimensions | `(32, 28, 28, 3) ‚Üí (32, 3, 28, 28)` |\n",
        "| **View** | `.view(shape)` | Reshape freely | `(32, 28, 28, 3) ‚Üí (32, 28*28*3)` |\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "My comments:\n",
        "Image data = (batch_size, channels, height, width) #Batch_size = samples, channels= the color of images RGB=3, Grey Scale=1, height and width for each image.\n",
        "#we used previously take channels and height and width to make an element as features(batch_size, features) then flatten then do nn.Linear,\n",
        "#But here we don't change the elements we pass them into conv2d then activation then pooling then flatten then Classification or regression (Fully Connected NN -the linear layer)\n",
        "\n",
        "batch_size = how many data we have(samples), features = how many columns\n",
        "---------\n",
        "how to handle images, play with dimensions:\n",
        "5 important functions:\n",
        "1. flatten: convert any shape to batch_size, features. use after all conv2d layers then flatten then nn.Linear then softmax or sigmoid\n",
        "2. squeeze:remove dimensions with size 1 even if it's batch_size, if the error shows that the shapes is (1, 32, 3, 28, 28)\n",
        "# that means we need to sqeeze to get rid off 1\n",
        "3. unsqueeze: add a dimension of size 1(the opposite of sqeeze) to increase the dimension 1\n",
        "4. permute: reorder or rearrange the dimensions (32, 28, 28, 3) ‚Üí permute(0, 3, 1, 2)\n",
        "# 0: dont change the place of batch_size value, 3: bring the channels to the second place, 1:the height bring it to the third place,\n",
        "# 2:the width bring it to the fourth place. ‚Üí (32, 3, 28, 28) this is how pytorch always read the dimensions:\n",
        " # (batch_size, channels, height, width) so permute help to make data arranged this way\n",
        "5. view: reshape in pytorch(except that reshape may result reshaping a copy or reshape the exact one )\n",
        "#(32, 28, 28, 3) ‚Üí view(-1, 28*28*3) ‚Üí(32, 28283),,, -1 means alll the batch_size and features= channels*weight*height\n",
        "\n"
      ],
      "metadata": {
        "id": "uSFC3BOxoB8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:26.524571Z",
          "iopub.status.busy": "2025-02-01T13:52:26.524183Z",
          "iopub.status.idle": "2025-02-01T13:52:30.375828Z",
          "shell.execute_reply": "2025-02-01T13:52:30.374699Z",
          "shell.execute_reply.started": "2025-02-01T13:52:26.524542Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5F0ztKduaE5",
        "outputId": "27fe00e5-7256-445c-e8b9-c178f7661a1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flatten: torch.Size([32, 2352])\n",
            "Squeeze: torch.Size([3, 28, 28])\n",
            "Unsqueeze: torch.Size([1, 3, 28, 28])\n",
            "Permute: torch.Size([32, 3, 28, 28])\n",
            "View: torch.Size([32, 2352])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1Ô∏è‚É£ Flatten - Convert any shape to (batch_size, features),\n",
        "x = torch.randn(32, 3, 28, 28)\n",
        "x_flat = x.flatten(start_dim=1)#The dimensions starts from 0, so we want to start from channel so we put 1 (batch_size, channels*height*widdth)= (0, 1*2*3)= (samples, features)\n",
        "print(\"Flatten:\", x_flat.shape)  # (32, 2352)\n",
        "\n",
        "# 2Ô∏è‚É£ Squeeze - Remove dimensions with size 1\n",
        "x = torch.randn(1, 3, 28, 28) #bring the x back again\n",
        "x_sq = x.squeeze()\n",
        "print(\"Squeeze:\", x_sq.shape)  # (3, 28, 28)\n",
        "\n",
        "# 3Ô∏è‚É£ Unsqueeze - Add a new dimension of size 1\n",
        "x = torch.randn(3, 28, 28)\n",
        "x_unsq = x.unsqueeze(0)\n",
        "print(\"Unsqueeze:\", x_unsq.shape)  # (1, 3, 28, 28)\n",
        "\n",
        "# 4Ô∏è‚É£ Permute - Reorder dimensions , batch remains the same\n",
        "x = torch.randn(32, 28, 28, 3)  # (batch, height, width, channels) the indices are (0,1,2,3) 0=batch 1=height 2=width 3=channels\n",
        "x_perm = x.permute(0, 3, 1, 2)  # (batch, channels, height, width),,,, tha channel is first after the batch#####\n",
        "print(\"Permute:\", x_perm.shape)  # (32, 3, 28, 28)\n",
        "\n",
        "# 5Ô∏è‚É£ View - Reshape freely while keeping same number of elements, close to flatten except you choose what to change,,,,, view=reshape they do the same thing it just another way.\n",
        "x = torch.randn(32, 28, 28, 3)\n",
        "x_view = x.view(32,-1)  # Flatten all except batch\n",
        "print(\"View:\", x_view.shape)  # (32, 28*28*3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BSXAKkuuaE7"
      },
      "source": [
        "### üîπ Changing Data Type or Moving Data/Model to CPU/GPU  \n",
        "\n",
        "PyTorch allows you to **change the datatype** of a tensor and **move it between CPU and GPU** using `.to()`.  \n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Change Datatype**\n",
        "Use `.to(dtype)` to convert a tensor's data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.377553Z",
          "iopub.status.busy": "2025-02-01T13:52:30.377089Z",
          "iopub.status.idle": "2025-02-01T13:52:30.397024Z",
          "shell.execute_reply": "2025-02-01T13:52:30.395751Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.377523Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-GIRolxuaE7",
        "outputId": "01910268-5eec-4e8e-f56f-d14c375c78de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.float16\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a float32 tensor\n",
        "x = torch.tensor([1.2, 2.3, 3.4], dtype=torch.float32)# tensor means ŸÖÿµŸÅŸàŸÅÿßÿ™\n",
        "print(x.dtype)  # Output: torch.float32\n",
        "\n",
        "# Convert to float16 or x.astype(torch.float16)\n",
        "x_half = x.to(torch.float16)#to function we can use it either to change the datatype or to change the device between CPU or GPU\n",
        "#if we want to change it to integer : x.to(torch.long)\n",
        "print(x_half.dtype)  # Output: torch.float16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5HTYd_5uaE7"
      },
      "source": [
        "### ‚úÖ **Move Tensors to GPU (if available)**\n",
        "Use `.to(device)` to move a tensor to GPU for faster computation.\n",
        "\n",
        "**GPUs are faster and more efficient** in most cases when training or inferencing deep learning models.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.399777Z",
          "iopub.status.busy": "2025-02-01T13:52:30.399396Z",
          "iopub.status.idle": "2025-02-01T13:52:30.410675Z",
          "shell.execute_reply": "2025-02-01T13:52:30.409468Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.399747Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCi7XbGAuaE8",
        "outputId": "e375b682-b639-4610-d6e4-5763bcecf4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Automatically select CPU or GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a tensor and move it to GPU\n",
        "x_gpu = x.to(device)\n",
        "print(x_gpu.device)  # Output: cuda:0 (if GPU is available) or cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvg6b557uaE8"
      },
      "source": [
        "*****IMPORTANT$$$$$$$ Note: When training a model, always\n",
        "move BOTH the model and data to the same device. Otherwise, you will get an error like this:\n",
        "\n",
        "`RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`\n",
        "\n",
        "\n",
        "THAT MEANS YOU FORGET (x.to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOUEjHMpuaE8"
      },
      "source": [
        "### üß† AI Layers\n",
        "\n",
        "PyTorch provides various **neural network layers** to build deep learning models. Below are some of the most commonly used layers.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1Ô∏è‚É£ Linear Layer (`nn.Linear`)\n",
        "\n",
        "### üìå **Usage**\n",
        "1. Used for **fully connected layers** (Dense layers).\n",
        "2. Typically used as the **final layer** in CNNs for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.412554Z",
          "iopub.status.busy": "2025-02-01T13:52:30.412185Z",
          "iopub.status.idle": "2025-02-01T13:52:30.463528Z",
          "shell.execute_reply": "2025-02-01T13:52:30.462470Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.412518Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGYE8dK9uaE8",
        "outputId": "5a9fc2bd-3ae8-4496-88ca-b130df02697e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.3215, -1.5391, -0.4777,  0.3489, -1.0123],\n",
            "        [ 0.5330, -0.0193, -0.0350, -0.8496, -0.9325],\n",
            "        [-0.6535,  0.8277, -0.2233,  0.2804,  0.8666],\n",
            "        [ 0.5195, -1.3002,  0.0297,  0.2345,  0.6802],\n",
            "        [-0.2172,  2.1970,  1.1002,  0.7686, -0.8665],\n",
            "        [ 0.0791, -1.1951,  0.0657,  1.0787,  0.4159],\n",
            "        [-0.2228, -0.2784,  0.8773, -0.2988, -0.8165],\n",
            "        [ 0.3773, -0.9491, -0.5993,  0.3754,  1.1734],\n",
            "        [-0.2324,  0.2299,  1.0320, -1.3078,  1.0531],\n",
            "        [ 0.0797,  0.9410,  0.8499,  0.7984,  0.0718],\n",
            "        [ 0.7790,  0.2712,  0.4804, -1.1975,  0.0832],\n",
            "        [ 0.1350, -0.4812,  0.5602, -0.8057,  0.7549],\n",
            "        [ 0.8890, -0.2294,  0.3046, -0.1650, -0.6700],\n",
            "        [-0.9126,  0.3510, -0.3198, -1.0591, -0.4240],\n",
            "        [ 0.4036, -1.5656, -1.3853,  1.4013, -0.2497],\n",
            "        [-0.3393, -0.5641, -1.4072, -0.3465,  0.6024]])\n",
            "Input Shape: torch.Size([16, 5])\n",
            "Output Shape: torch.Size([16, 3])\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a Linear layer\n",
        "linear_layer = nn.Linear(in_features=5, out_features=3)#care about channels here only so they are RGB=3\n",
        "\n",
        "# Random input tensor (batch_size=16, in_features=5)\n",
        "x = torch.randn(16, 5) #in linear layer batch size first then features\n",
        "print(x)\n",
        "\n",
        "# Forward pass\n",
        "output = linear_layer(x)\n",
        "\n",
        "print(\"Input Shape:\", x.shape)       # (16, 5)\n",
        "print(\"Output Shape:\", output.shape)  # (16, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ-BMrI0uaE8"
      },
      "source": [
        "## üîπ 2Ô∏è‚É£ Convolutional Layer (`nn.Conv2d`)\n",
        "\n",
        "\n",
        "###  üìå **Usage**\n",
        "‚úÖ `nn.Conv2d` is used for **feature extraction** in images.  \n",
        "üö´ It **does not perform classification or regression**‚Äîyou need a `nn.Linear` layer for that.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.465080Z",
          "iopub.status.busy": "2025-02-01T13:52:30.464719Z",
          "iopub.status.idle": "2025-02-01T13:52:30.547569Z",
          "shell.execute_reply": "2025-02-01T13:52:30.546561Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.465047Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qtoYGPkuaE9",
        "outputId": "99748440-0de9-4acb-fcd9-1a9186156f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: torch.Size([16, 3, 32, 32])\n",
            "Output Shape: torch.Size([16, 16, 30, 30])\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a Conv2D layer\n",
        "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3) #RGB -> channels is 3, Black&White -> channels is , out_channels if it colored will be 3 if not 16 usually CNN has increased channels dimension 16, 32, 64,\n",
        "# but in Linear Layer is the opposite the out_channel is smaller.\n",
        "\n",
        "# Random input tensor (batch_size=16, channels=3, height=32, width=32)\n",
        "x = torch.randn(16, 3, 32, 32)\n",
        "\n",
        "# Forward pass\n",
        "output = conv_layer(x)\n",
        "\n",
        "print(\"Input Shape:\", x.shape)       # (16, 3, 32, 32)\n",
        "print(\"Output Shape:\", output.shape)  # (16, 16, 30, 30) (batch_size, out_channels ,height, width ) #calculate it using the feature map formula\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjgYLCPUuaE9"
      },
      "source": [
        "# **üìå Important Note**\n",
        "When you use **Convolutional layers (`nn.Conv2d`)**, the **output channels tend to be larger** than the input channels (unlike `nn.Linear`).  \n",
        "\n",
        "### **Why?**\n",
        "- Each convolutional layer **extracts more useful features** from the input.\n",
        "- As more filters are applied, the **number of channels increases**.\n",
        "- Meanwhile, **spatial size (height & width) decreases**.\n",
        "\n",
        "‚úÖ **This allows the model to capture richer features while reducing unnecessary spatial details.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M66M_LbBuaE9"
      },
      "source": [
        "The **output image size** after a convolutional layer is calculated using the formula (or you can get it by trial-and-error):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgnqCOXMuaE9"
      },
      "source": [
        "![image.png](https://i.imgur.com/8XKBFBU.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTY4NoOXuaE9"
      },
      "source": [
        "## üîπ 3Ô∏è‚É£ Pooling (`nn.MaxPool2d` / `nn.AvgPool2d`)**\n",
        "\n",
        "###  üìå **Usage**\n",
        "- **Reduces spatial dimensions** (height & width) while retaining important features.\n",
        "- Typically used **after Conv2D layers** to downsample feature maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.548925Z",
          "iopub.status.busy": "2025-02-01T13:52:30.548526Z",
          "iopub.status.idle": "2025-02-01T13:52:30.569245Z",
          "shell.execute_reply": "2025-02-01T13:52:30.568050Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.548885Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zul-le55uaE9",
        "outputId": "3473c347-ad2b-410b-a079-af263d445a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Conv2D: torch.Size([2, 16, 32, 32])\n",
            "After ReLU: torch.Size([2, 16, 32, 32])\n",
            "After Pooling: torch.Size([2, 16, 16, 16])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define layers\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)#same padding\n",
        "relu = nn.ReLU()    ## An old friend ;)     Do you remember why do we need it here? for adding non-linearity\n",
        "pool = nn.MaxPool2d(kernel_size=2, stride=2)#reduce size of the image\n",
        "\n",
        "# Sample input tensor (batch_size=2, channels=3, height=32, width=32)\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "\n",
        "\"\"\"\n",
        "1 2 3 4\n",
        "1 2 3 4\n",
        "1 2 3 4\n",
        "1 2 3 4\n",
        "\"\"\"\n",
        "# üîπ Step 1: Convolution\n",
        "x = conv(x)\n",
        "print(\"After Conv2D:\", x.shape)  # (2, 16, 32, 32)the channel increase due to conv2d\n",
        "\n",
        "# üîπ Step 2: ReLU Activation\n",
        "x = relu(x)\n",
        "print(\"After ReLU:\", x.shape)  # (2, 16, 32, 32)\n",
        "\n",
        "# üîπ Step 3: Pooling (Reduces spatial size)\n",
        "x = pool(x)\n",
        "print(\"After Pooling:\", x.shape)  # (2, 16, 16, 16)#the image size reduced in pooling\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "conv2d : increase the number of channels\n",
        "pooling : reduce the size of images (height, width)\n",
        "ReLu : add non-linearity to the features\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNk5K8Y3uaE-"
      },
      "source": [
        "### **But wait a minute, if Conv2D extracts the features only, how should we do the classification/regression? ü§î**\n",
        "\n",
        "Conv2D layers are responsible for extracting features (edges, textures, patterns, etc.) from the input data, but they **do not perform classification or regression directly**.  \n",
        "\n",
        "To classify or predict, we need to **map the extracted features** to the desired output using **fully connected layers (`nn.Linear`)** after flattening the feature maps.\n",
        "\n",
        "---\n",
        "\n",
        "### **üèóÔ∏è CNN Structure Example**\n",
        "A typical CNN model for classification or regression follows this pattern:\n",
        "\n",
        "1Ô∏è‚É£ **`nn.Conv2d`** ‚Üí Extracts features .  \n",
        "2Ô∏è‚É£ **`nn.MaxPool2d`** ‚Üí Reduces feature map size to focus on important information.  \n",
        "3Ô∏è‚É£ **`nn.Conv2d`** ‚Üí Extracts more features.  \n",
        "\n",
        "... Add more layers if you want\n",
        "\n",
        "4Ô∏è‚É£ **Flatten & `nn.Linear`** ‚Üí Maps extracted features to output classes or predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U1bKDZKuaE-"
      },
      "source": [
        "![image.png](https://i.imgur.com/fwNdXJs.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrQaYam6uaE-"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### **üîπ Example: Classification with CNN**\n",
        "| **Layer**                  | **Purpose**                        | **Example Shape Transformation** |\n",
        "|----------------------------|-------------------------------------|-----------------------------------|\n",
        "| **Input Image**            | Raw input                          | `(batch_size, channels, height, width)` |\n",
        "| **`nn.Conv2d`**            | Extract features                   | `(32, 3, 32, 32) ‚Üí (32, 16, 30, 30)` |\n",
        "| **`nn.MaxPool2d`**         | Downsample feature maps            | `(32, 16, 30, 30) ‚Üí (32, 16, 15, 15)` |\n",
        "| **`nn.Conv2d`**            | Extract more features              | `(32, 16, 15, 15) ‚Üí (32, 32, 13, 13)` |\n",
        "| **Flatten**                | Prepare for fully connected layers | `(32, 32, 13, 13) ‚Üí (32, 32*13*13)` |\n",
        "| **`nn.Linear`**            | Map features to classes/predictions| `(32, 32*13*13) ‚Üí (32, 10)` (for 10 classes) |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:30.571030Z",
          "iopub.status.busy": "2025-02-01T13:52:30.570501Z",
          "iopub.status.idle": "2025-02-01T13:52:30.631073Z",
          "shell.execute_reply": "2025-02-01T13:52:30.629954Z",
          "shell.execute_reply.started": "2025-02-01T13:52:30.570988Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoeQYQX4uaE-",
        "outputId": "f2afe942-7e43-4c3a-c82a-04a7ffce6fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Conv2D: torch.Size([2, 16, 30, 30])\n",
            "After ReLU: torch.Size([2, 16, 30, 30])\n",
            "After Pooling: torch.Size([2, 16, 15, 15])\n",
            "After Flatten: torch.Size([2, 3600])\n",
            "After Linear (Logits): torch.Size([2, 10])\n",
            "After Softmax (Probabilities): tensor([0.0890, 0.0669, 0.1922, 0.1198, 0.1254, 0.0540, 0.0289, 0.0952, 0.1651,\n",
            "        0.0636], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "### STEP2:Model class: ###\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#### Define layers\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)  # Conv2D layer\n",
        "relu = nn.ReLU()  # Activation function, the size now  16, 30 ,30\n",
        "pool = nn.MaxPool2d(kernel_size=2)  # Pooling layer\n",
        "flatten = nn.Flatten()  # Flatten layer to prepare for Linear\n",
        "linear = nn.Linear(16 * 15 * 15, 10)  # Fully connected layer for classification (10 classes), 0 to 9\n",
        "#channel*height*width\n",
        "softmax = nn.Softmax(dim=1)  # ## Another old friend ;),,,,, sigmoid for...\n",
        "\n",
        "# Sample input tensor (batch_size=2, channels=3, height=32, width=32)\n",
        "x = torch.randn(2, 3, 32, 32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### USE THE LAYERS\n",
        "\n",
        "\n",
        "\n",
        "# üîπ Step 1: Convolution\n",
        "x = conv(x)\n",
        "print(\"After Conv2D:\", x.shape)  # (2, 16, 30, 30), channel increases was # Sample input tensor (batch_size=2, channels=3, height=32, width=32)\n",
        "# 32 the size then 32-2=30 the padding not written but it affect the size ,\n",
        "#If i dont want to reduce the size by 2(height, width) add at conv = padding =1 to make no change in the dimensions of the images\n",
        "\n",
        "\n",
        "# üîπ Step 2: ReLU Activation\n",
        "x = relu(x)\n",
        "print(\"After ReLU:\", x.shape)  # (2, 16, 30, 30)\n",
        "\n",
        "# üîπ Step 3: Pooling (Reduces spatial size)\n",
        "x = pool(x)\n",
        "print(\"After Pooling:\", x.shape)  # (2, 16, 15, 15)\n",
        "\n",
        "# üîπ Step 4: Flatten (Convert to batch_size, features)\n",
        "x = flatten(x)\n",
        "print(\"After Flatten:\", x.shape)  # (2, 16*15*15)\n",
        "\n",
        "# üîπ Step 5: Fully Connected Layer\n",
        "x = linear(x) # the input here is the feature after flatten nn.Linear(16 * 15 * 15, 10),,,, 15 you can know it by replacing the number with randoms as(2789345759t8345)trillion,\n",
        "#then print the shape ,then you will get an error that tells you the shape of the images\n",
        "print(\"After Linear (Logits):\", x.shape)  # (2, 10) ‚Üí 10 values, one per class\n",
        "\n",
        "# üîπ Step 6: Softmax (Convert logits(16 * 15 * 15, 10)(numbers that before softmax) to probabilities)\n",
        "x = softmax(x)# WHY??? for probability,, Mean Squared Error(MSE)?? for regression ,, softmax then loss is CrossEntropyLoss ??? multi classification\n",
        "#Do we need softmax to do classification?! no if we use CrossEntropyLoss it apply softmax internally\n",
        "print(\"After Softmax (Probabilities):\", x[0])  # Probabilities for each class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KeDnlshuaE-"
      },
      "source": [
        "# **Great! Now you know how to build a CNN model!**\n",
        "\n",
        "However, PyTorch has a specific structure to organize your workflow.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå PyTorch Workflow Organization**\n",
        "\n",
        "### **It consists of 4 main components:**\n",
        "1Ô∏è‚É£ **Dataset Class**  \n",
        "- Handles loading and preprocessing data.  \n",
        "- Converts raw data (e.g., images, CSVs) into model-ready tensors.  \n",
        "\n",
        "2Ô∏è‚É£ **Model Class**  \n",
        "- Defines the architecture of your neural network (e.g., layers, activations).  \n",
        "\n",
        "3Ô∏è‚É£ **Training Loop**  \n",
        "- Updates model weights using backpropagation and optimizers.  \n",
        "- Computes the loss for every batch and adjusts the parameters to minimize it.  \n",
        "\n",
        "4Ô∏è‚É£ **Validation Loop**  \n",
        "- Evaluates the model's performance on a validation set.  \n",
        "- Does not update weights but computes metrics like accuracy or loss.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Note:**\n",
        "All the labs will follow this structure. You will just modify the content for different tasks, such as changing datasets, architectures, or loss functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-osS8qBuaE_"
      },
      "source": [
        "# **üìå Dataset Class**\n",
        "\n",
        "- The **Dataset Class** is designed to **load and preprocess only one sample** at a time.\n",
        "- The **DataLoader** uses the Dataset Class to load **multiple samples (batches)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wu0jo3juaE_"
      },
      "source": [
        "## **1Ô∏è‚É£ It Could Be Ready-to-Use:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Dq9e-fuaE_",
        "outputId": "1bcb56e1-7947-4b8f-8a90-e4d038c55c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 16.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 496kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 4.47MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 11.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Ready data\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "train_dataset = MNIST(root=\"./data\", train=True, download=True)\n",
        "test_dataset = MNIST(root=\"./data\", train=False, download=True)\n",
        "\n",
        "#Or write it by yourself:\n",
        "\n",
        "for torchvision.datasets import CIFAR10\n",
        "\n",
        "train_dataset = CIFAR10(root=\"./data\", train=True, download=True)\n",
        "test_dataset = CIFAR10(root=\"./data\", train=False, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define a transform to convert PIL image to tensor , we need it to be tensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Get the first sample\n",
        "image, label = train_dataset[0]\n",
        "\n",
        "# Convert PIL image to tensor\n",
        "image_tensor = transform(image)\n",
        "\n",
        "# Print the shape of the tensor\n",
        "print(\"Image Tensor Shape:\", image_tensor.shape) #Image Tensor Shape: torch.Size([1, 28, 28]) , Label: 5,,, since the channel is =1 that means the images is grey scale\n",
        "print(\"Label:\", label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6vwObEa8KeI",
        "outputId": "fc68deb3-59cc-404e-a84c-bd6458ec54a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Tensor Shape: torch.Size([1, 28, 28])\n",
            "Label: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaP_5eVRuaE_"
      },
      "source": [
        "## **2Ô∏è‚É£ Or You Have to Define It Yourself (We will explore how to define it in another lab).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T210c0zXuaE_"
      },
      "source": [
        "# **üìå Model Class**\n",
        "---\n",
        "\n",
        "## **üìå Key Components:**\n",
        "1Ô∏è‚É£ **Define Layers (`__init__` method):**  \n",
        "- Use PyTorch modules (e.g., `nn.Conv2d`, `nn.Linear`) to create the model's architecture.  \n",
        "\n",
        "2Ô∏è‚É£ **Forward Pass (`forward` method):**  \n",
        "- Specify how the input should be fed through the layers step by step.  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1nU7O5JuaE_"
      },
      "source": [
        "## **1Ô∏è‚É£ You may define It Yourself:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:45.676080Z",
          "iopub.status.busy": "2025-02-01T13:52:45.675543Z",
          "iopub.status.idle": "2025-02-01T13:52:45.683583Z",
          "shell.execute_reply": "2025-02-01T13:52:45.682535Z",
          "shell.execute_reply.started": "2025-02-01T13:52:45.676041Z"
        },
        "trusted": true,
        "id": "SBqOTo4xuaE_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        1Ô∏è‚É£ Define all layers in the model.\n",
        "        \"\"\"\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        # Convolutional Layer + Activation + Pooling\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1) #out_channnel= usually you choose it 16 or 32 so 28*28\n",
        "        #TODO    #You remembered something?\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        #What is the expected size after doing the maxpooling? Check above to know the original size\n",
        "        self.fc = nn.Linear(16 * 14 * 14, 10)  # Output 10 classes,#14*14 using the formula\n",
        "\n",
        "        # ???? Layer\n",
        "        #TODO. #Which layer is this , multiclass sp we use Softamx\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x): #now we use them not only determine them\n",
        "        \"\"\"\n",
        "        2Ô∏è‚É£ Define the forward pass (how data flows through the model).\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)  # Convolution\n",
        "        x = self.relu(x)#TODO  # Activation function?\n",
        "        x = self.pool(x)  # Pooling\n",
        "        x = x.view(-1, 16*14*14 )#TODO#What should we do before using fully connected layer? -1 for whatevere input batching,\n",
        "        # 16*14*14 16 is the number of out_channels and 14,14 from the formula\n",
        "        x = self.fc(x)  # Fully connected layer\n",
        "        x = self.softmax(x)#TODO  # Convert logits to probabilities\n",
        "        return x  # Returns probability distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T78pFuTfuaE_"
      },
      "source": [
        "###\n",
        "## **2Ô∏è‚É£ Or It Could Be Given Ready-to-Use (Will explore in another lab):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true,
        "id": "6SUn3kGFuaFA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b05e3ca-6f9c-4064-b265-c67944813961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Example: pretrained model\n",
        "import torchvision.models as models\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "\n",
        "\n",
        "### for our custom model:\n",
        "#1:initialize an object, 2:use it inside the loop\n",
        "model = CustomModel()# add parameters if any\n",
        "pred = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhoCUhOXuaFA"
      },
      "source": [
        "# **üìå Training Loop**\n",
        "\n",
        "## **What is the Training Loop?**\n",
        "The **training loop** is responsible for **updating the model's weights** so that it learns to minimize the loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:46.279176Z",
          "iopub.status.busy": "2025-02-01T13:52:46.278877Z",
          "iopub.status.idle": "2025-02-01T13:52:46.284659Z",
          "shell.execute_reply": "2025-02-01T13:52:46.283750Z",
          "shell.execute_reply.started": "2025-02-01T13:52:46.279148Z"
        },
        "trusted": true,
        "id": "Vhq3Aa_kuaFA"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()  # Set model to training mode, you will understand why later# training mode: turn on dropout and batch normalization for the training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    # loop for steps ####\n",
        "    for images, labels in dataloader:\n",
        "\n",
        "        images, labels = images.to(device) , labels.to(device)#TODO  #We don't want to train in the CPU right? so what we do here\n",
        "\n",
        "\n",
        "        outputs = model(images)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "\n",
        "\n",
        "        #backProbogation steps from stage2\n",
        "        optimizer.zero_grad()  # Reset , forget gradients\n",
        "        loss.backward()  # Backpropagation (compute gradients)\n",
        "        optimizer.step()  # Update model parameters, update weight with gradients that we just calculated in loss.backward()\n",
        "\n",
        "        # Collect the loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)  # Return average loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#differences: batche = [1, 2, 3, 4, 5]\n",
        "#Step: each batch you take inside the loop is one size (each time you pass a batch into the loop)\n",
        "#Epoch: you passed all the data if the epoch is 20 and we have 5 batches,\n",
        "#it will run them 10 times but with reorder(shuffle the batches)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn_yq_KfuaFA"
      },
      "source": [
        "# **üìå Validation Loop**\n",
        "\n",
        "## **What is the Validation Loop?**\n",
        "- The **validation loop** is used to **evaluate model performance** on unseen data.  \n",
        "- Unlike the training loop, **it does NOT update the model‚Äôs weights**.  \n",
        "- It helps track **loss and accuracy** to monitor model improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:46.286262Z",
          "iopub.status.busy": "2025-02-01T13:52:46.285804Z",
          "iopub.status.idle": "2025-02-01T13:52:46.304261Z",
          "shell.execute_reply": "2025-02-01T13:52:46.303139Z",
          "shell.execute_reply.started": "2025-02-01T13:52:46.286150Z"
        },
        "trusted": true,
        "id": "K3pllZKquaFB"
      },
      "outputs": [],
      "source": [
        "#### Doesn't update the weights, we can calculate the accuracy\n",
        "\n",
        "def validate(model, dataloader, criterion, device): #something wrong######\n",
        "    model.eval() #evaluation mode: turn on dropout and batch normalization for the evaluation mode\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad(): #Disable gradient calculation\n",
        "        for images, labels in dataloader: #every iteration gives us a batch the image and it's label\n",
        "            #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #I added it\n",
        "            images, labels = images.to(device), labels.to(device)#TODO  #Both data should be in the same device right?, my addition\n",
        "\n",
        "\n",
        "            outputs = model(images)\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "\n",
        "    #another way van be here\n",
        "    return 100 * correct / total  # Return accuracy\n",
        "\n",
        "''' Another way to write the accuracy in percentage inside the for loop as:\n",
        "avg_loss = total_loss / len(dataloader) # calculate the loss\n",
        "accuracy = 100 * correct / total\n",
        "return avg_loss, accuracy\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "#### any dataset class is designed to read one sample only from the data.\n",
        "#### DataLoader take the dataset class and read many samples at a time by specifying the batch_size=64 for example(use more GPU so reduce it if you have error),\n",
        "#and do shuffle in train data only.\n",
        "\n",
        "#  EX\n",
        "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsZE8FuIuaFB"
      },
      "source": [
        "# **üìå Full Training Process in PyTorch**\n",
        "\n",
        "Now that you understand the **Dataset Class, Model Class, Training Loop, and Validation Loop**, it's time to put everything together into a **full training process**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-01T13:52:46.305727Z",
          "iopub.status.busy": "2025-02-01T13:52:46.305338Z",
          "iopub.status.idle": "2025-02-01T13:54:13.696582Z",
          "shell.execute_reply": "2025-02-01T13:54:13.695558Z",
          "shell.execute_reply.started": "2025-02-01T13:52:46.305698Z"
        },
        "trusted": true,
        "id": "Dp1xHoRpuaFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0646b498-9321-4a23-df78-d461447a49be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Validation Accuracy = 92.26%\n",
            "Epoch 2: Validation Accuracy = 94.50%\n",
            "Epoch 3: Validation Accuracy = 96.33%\n",
            "Epoch 4: Validation Accuracy = 96.84%\n",
            "Epoch 5: Validation Accuracy = 97.27%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "#Put the data in dataloader\n",
        "#Why? will know in another lab be patient\n",
        "# üîπ Load MNIST Dataset\n",
        "transform = transforms.ToTensor()\n",
        "train_dataset = MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "#Model class :\n",
        "#Train loop : images and labels on to(device)\n",
        "#Validation loop: images and labels on to(device)\n",
        "\n",
        "\n",
        "# Run Training\n",
        "model = CustomModel()###take it from previous cells\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #TODO  #The model will go to device.  but which device? the same as the data before\n",
        "model.to(device) #And the model too on to(device) such as the images and labels in both training, validatio loops.\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()#TODO  #Which loss function used here? Hint: this is Multi-Class Classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0008)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#loop for epochs #####\n",
        "for epoch in range(5):  # Train for 5 epochs\n",
        "    train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    accuracy = validate(model, test_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}: Validation Accuracy = {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsK2tkxWuaFC"
      },
      "source": [
        "![image.png](https://i.imgur.com/1xbDOQX.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPmesb5JuaFC"
      },
      "source": [
        "### Contributed by: Mohamed Eltayeb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAfYpl5AuaFC"
      },
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30839,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}