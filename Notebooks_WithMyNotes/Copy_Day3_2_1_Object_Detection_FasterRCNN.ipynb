{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "936c6c87",
      "metadata": {
        "id": "936c6c87"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "824b7295",
      "metadata": {
        "id": "824b7295"
      },
      "source": [
        "# **üöÄ Object Detection with Faster R-CNN**\n",
        "In this lab, we will:\n",
        "\n",
        "‚úÖ Build a **custom Dataset class** for **Pascal VOC dataset**  \n",
        "‚úÖ Use a **pretrained Faster R-CNN model** for object detection  \n",
        "‚úÖ Train and evaluate the model  \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df1282f",
      "metadata": {
        "id": "8df1282f"
      },
      "source": [
        "## **1Ô∏è‚É£ Dataset Class**\n",
        "We use the **Pascal VOC 2007 dataset**, which contains images with **bounding boxes and labels**.  \n",
        "PyTorch provides a built-in dataset loader: `torchvision.datasets.VOCDetection`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f60e0b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f60e0b8",
        "outputId": "ae3c40b0-4110-47a0-dc6c-b0e9665e23b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar to ./VOC_data/VOCtrainval_06-Nov-2007.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460M/460M [00:15<00:00, 30.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./VOC_data/VOCtrainval_06-Nov-2007.tar to ./VOC_data/\n",
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar to ./VOC_data/VOCtest_06-Nov-2007.tar\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 451M/451M [00:17<00:00, 26.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./VOC_data/VOCtest_06-Nov-2007.tar to ./VOC_data/\n"
          ]
        }
      ],
      "source": [
        "### **üîπ Load & Transform Dataset**\n",
        "import torchvision\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "# Define dataset path and transformations\n",
        "data_path = \"./VOC_data/\"\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "# Load Pascal VOC dataset (train & test)\n",
        "train_dataset = VOCDetection(root=data_path, year='2007', image_set='train', download=True, transform=transform)\n",
        "test_dataset = VOCDetection(root=data_path, year='2007', image_set='test', download=True, transform=transform)\n",
        "\n",
        "# Custom collate function to handle variable number of bounding boxes in a single image\n",
        "def custom_collate_fn(data):\n",
        "    return tuple(zip(*data))\n",
        "\n",
        "# Create DataLoaders                 (Tip: Use lower batch size if encountered Out-of-Memory (OOM) error in Training)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TJ9sC8SKsNtt",
      "metadata": {
        "id": "TJ9sC8SKsNtt"
      },
      "outputs": [],
      "source": [
        "#my added code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "class AnnotationDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.image_files = [f for f in os.listdir(img_dir) if f.endswith((\".jpg\"))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        label_path = os.path.join(self.label_dir, img_name.replace(\".jpg\", \".xml\"))\n",
        "\n",
        "        image = read_image(img_path)\n",
        "        #assuming function to read images\n",
        "        label = self.parse_annotation(label_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image, label = self.transform(image, label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def parse_annotation(self, label_path):\n",
        "        # Parse the XML annotation file\n",
        "        tree = ET.parse(label_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        # Example of extracting class labels (adjust based on your dataset structure)\n",
        "        labels = []\n",
        "        for obj in root.findall(\"object\"):\n",
        "            class_name = obj.find(\"name\").text\n",
        "            labels.append(class_name)\n",
        "\n",
        "        return labels\n",
        "\n",
        "dataset = AnnotationDataset(img_dir=\"/content/VOC_data/VOCdevkit/VOC2007/JPEGImages\", label_dir=\"/content/VOC_data/VOCdevkit/VOC2007/Annotations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HlJaXU4jttaO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlJaXU4jttaO",
        "outputId": "fa13e44d-ae66-431e-ee0b-f84671216d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 500, 375]) ['cow', 'cow', 'cow', 'cow']\n"
          ]
        }
      ],
      "source": [
        "image, label = dataset[10]\n",
        "print(image.shape, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_UkKxbd_PxKB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UkKxbd_PxKB",
        "outputId": "52b30e58-2b09-4e56-fbd7-b8ee83b9aa77"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset VOCDetection\n",
              "    Number of datapoints: 2501\n",
              "    Root location: ./VOC_data/\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "           )"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7XlIgv-bOz5Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XlIgv-bOz5Q",
        "outputId": "98ec8016-18c6-47f2-f255-f48278ce72b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'annotation': {'folder': 'VOC2007',\n",
              "  'filename': '000012.jpg',\n",
              "  'source': {'database': 'The VOC2007 Database',\n",
              "   'annotation': 'PASCAL VOC2007',\n",
              "   'image': 'flickr',\n",
              "   'flickrid': '207539885'},\n",
              "  'owner': {'flickrid': 'KevBow', 'name': '?'},\n",
              "  'size': {'width': '500', 'height': '333', 'depth': '3'},\n",
              "  'segmented': '0',\n",
              "  'object': [{'name': 'car',\n",
              "    'pose': 'Rear',\n",
              "    'truncated': '0',\n",
              "    'difficult': '0',\n",
              "    'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5O5z16OuMVmE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O5z16OuMVmE",
        "outputId": "b0710370-04a8-43a4-f57d-1bf563867004"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[[0.2706, 0.2588, 0.2627,  ..., 0.2157, 0.2118, 0.2000],\n",
              "          [0.2824, 0.2706, 0.2627,  ..., 0.2078, 0.1961, 0.2078],\n",
              "          [0.2549, 0.2627, 0.2667,  ..., 0.2353, 0.2353, 0.2275],\n",
              "          ...,\n",
              "          [0.3020, 0.3059, 0.3098,  ..., 0.3294, 0.3333, 0.3294],\n",
              "          [0.3137, 0.3176, 0.3216,  ..., 0.3059, 0.3098, 0.3098],\n",
              "          [0.3216, 0.3255, 0.3255,  ..., 0.3137, 0.3098, 0.3098]],\n",
              " \n",
              "         [[0.2706, 0.2588, 0.2627,  ..., 0.2235, 0.2196, 0.2078],\n",
              "          [0.2824, 0.2706, 0.2627,  ..., 0.2157, 0.2039, 0.2157],\n",
              "          [0.2549, 0.2627, 0.2667,  ..., 0.2431, 0.2431, 0.2353],\n",
              "          ...,\n",
              "          [0.2980, 0.3020, 0.3059,  ..., 0.3294, 0.3333, 0.3294],\n",
              "          [0.3098, 0.3137, 0.3176,  ..., 0.3059, 0.3098, 0.3098],\n",
              "          [0.3176, 0.3216, 0.3216,  ..., 0.3176, 0.3098, 0.3098]],\n",
              " \n",
              "         [[0.2627, 0.2510, 0.2549,  ..., 0.2196, 0.2157, 0.2039],\n",
              "          [0.2745, 0.2627, 0.2549,  ..., 0.2118, 0.2000, 0.2118],\n",
              "          [0.2471, 0.2549, 0.2588,  ..., 0.2392, 0.2392, 0.2314],\n",
              "          ...,\n",
              "          [0.2824, 0.2863, 0.2980,  ..., 0.3216, 0.3255, 0.3216],\n",
              "          [0.2941, 0.2980, 0.3098,  ..., 0.2980, 0.3020, 0.3020],\n",
              "          [0.3020, 0.3059, 0.3137,  ..., 0.2980, 0.3020, 0.3020]]]),\n",
              " {'annotation': {'folder': 'VOC2007',\n",
              "   'filename': '000012.jpg',\n",
              "   'source': {'database': 'The VOC2007 Database',\n",
              "    'annotation': 'PASCAL VOC2007',\n",
              "    'image': 'flickr',\n",
              "    'flickrid': '207539885'},\n",
              "   'owner': {'flickrid': 'KevBow', 'name': '?'},\n",
              "   'size': {'width': '500', 'height': '333', 'depth': '3'},\n",
              "   'segmented': '0',\n",
              "   'object': [{'name': 'car',\n",
              "     'pose': 'Rear',\n",
              "     'truncated': '0',\n",
              "     'difficult': '0',\n",
              "     'bndbox': {'xmin': '156', 'ymin': '97', 'xmax': '351', 'ymax': '270'}}]}})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hNniLEyhMVN2",
      "metadata": {
        "id": "hNniLEyhMVN2"
      },
      "outputs": [],
      "source": [
        "'''for i in range(len(train_dataset)):\n",
        "    sample = train_dataset[i]  # Access each sample\n",
        "    print(sample)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63daca94",
      "metadata": {
        "id": "63daca94"
      },
      "outputs": [],
      "source": [
        "# Mappings of label names (found in dataset annotation) to integer IDs (or classes) which we will feed to the model\n",
        "voc_classes = {\n",
        "    \"aeroplane\": 0,\n",
        "    \"bicycle\": 1,\n",
        "    \"bird\": 2,\n",
        "    \"boat\": 3,\n",
        "    \"bottle\": 4,\n",
        "    \"bus\": 5,\n",
        "    \"car\": 6,\n",
        "    \"cat\": 7,\n",
        "    \"chair\": 8,\n",
        "    \"cow\": 9,\n",
        "    \"diningtable\": 10,\n",
        "    \"dog\": 11,\n",
        "    \"horse\": 12,\n",
        "    \"motorbike\": 13,\n",
        "    \"person\": 14,\n",
        "    \"pottedplant\": 15,\n",
        "    \"sheep\": 16,\n",
        "    \"sofa\": 17,\n",
        "    \"train\": 18,\n",
        "    \"tvmonitor\": 19,\n",
        "}\n",
        "\n",
        "#  Reverse of label to class id mapping. needed because the model predictions will be ids and we need to change it to label to visualize it.\n",
        "reverse_voc_classes = {v: k for k, v in voc_classes.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7efd317",
      "metadata": {
        "id": "a7efd317"
      },
      "source": [
        "### **üîπ Why Do We Need a Custom `collate_fn`?**\n",
        "Unlike classification datasets, where each image has a **fixed shape and label**, object detection images have **variable numbers of bounding boxes**.  \n",
        "\n",
        "- The default `collate_fn` (which applies `torch.stack`) **doesn't work**, since bounding box tensors have different shapes.  \n",
        "- Instead, we **return a tuple** that **keeps individual image-label pairs separate**.\n",
        "\n",
        "##### Before using custom collate_fn:\n",
        "```python\n",
        "data = [\n",
        "    (image1, dict1),  \n",
        "    (image2, dict2),\n",
        "    ...  \n",
        "]\n",
        "```\n",
        "##### After:\n",
        "```python\n",
        "images_tuple = (image1, image2, ...)  \n",
        "targets_tuple = (dict1, dict2, ...)   \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47cd40bf",
      "metadata": {
        "id": "47cd40bf"
      },
      "source": [
        "## **2Ô∏è‚É£ Model Class**\n",
        "We use a **pretrained Faster R-CNN with a MobileNetV3-Large backbone**.\n",
        "\n",
        "### **üîπ Modify the Model**\n",
        "- The default model is trained on **COCO dataset** with **91 classes**.\n",
        "- We modify the classifier to detect **20 Pascal VOC classes**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "524ed7ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "524ed7ea",
        "outputId": "e94d2fe0-9368-4477-c44e-13ccbd1cd60a",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_mobilenet_v3_large_fpn-fb6a3cc7.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 74.2M/74.2M [00:00<00:00, 126MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "        (2): Hardswish()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
              "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): Conv2dNormActivation(\n",
              "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "        (2): Hardswish()\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-1): 2 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=20, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=80, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "\n",
        "# Load pretrained Faster R-CNN model\n",
        "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "\n",
        "# Change number of output classes to match Pascal VOC dataset\n",
        "num_classes = 20  # Pascal VOC has 20 object classes\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features  # Input features for predictor\n",
        "\n",
        "# Replace final layer with new predictor\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "\n",
        "# Freeze the backbone and just finetune the head (You can finetune the whole model, but it'd take time and resources)\n",
        "model.requires_grad_(False)\n",
        "model.roi_heads.box_predictor = model.roi_heads.box_predictor.requires_grad_(True)\n",
        "\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def5003f",
      "metadata": {
        "id": "def5003f"
      },
      "source": [
        "## **3Ô∏è‚É£ Training and Validation Loops**\n",
        "### **üîπ Training Loop**\n",
        "- The model takes **images & targets** and computes **losses internally**. No need to define the loss.\n",
        "- We only need to **backpropagate and update the optimizer**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed05f2b",
      "metadata": {
        "id": "6ed05f2b"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for images, targets in tqdm(dataloader):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        # Convert targets\n",
        "        for target in targets:\n",
        "            boxes = []\n",
        "            labels = []\n",
        "            for obj in target['annotation']['object']:\n",
        "                label = obj['name']\n",
        "                box = obj['bndbox']\n",
        "                xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "                boxes.append(torch.Tensor([xmin, ymin, xmax, ymax]).to(device))\n",
        "                labels.append(voc_classes[label])\n",
        "\n",
        "            target['boxes'] = torch.stack(boxes)\n",
        "            target['labels'] = torch.Tensor(labels).type(torch.int64).to(device)\n",
        "\n",
        "        # Compute losses\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())  # Sum all losses\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c407612",
      "metadata": {
        "id": "4c407612"
      },
      "source": [
        "### **üîπ Validation Loop**\n",
        "#### **üîπ How Do We Evaluate Object Detection Models?**\n",
        "To evaluate object detection models like **Faster R-CNN**, we need to measure **how well the predicted bounding boxes match the ground truth boxes**.\n",
        "\n",
        "![image.png](https://i.imgur.com/MDFxFMX.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìå Intersection over Union (IoU)**\n",
        "‚úÖ We consider a detection **correct** if the predicted box **overlaps significantly** with the ground truth box.  \n",
        "‚úÖ This is measured using **Intersection over Union (IoU)**, which calculates the **ratio of overlap** between the two boxes.\n",
        "\n",
        "$$\n",
        "IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
        "$$\n",
        "\n",
        "üöÄ **Higher IoU = Better Detection!**  \n",
        "\n",
        "\n",
        "![image.png](https://i.imgur.com/yNNhjwr.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### **üìå What is mAP@0.5:0.95?**\n",
        "mAP (**mean Average Precision**) is the most commonly used **metric for object detection**.\n",
        "\n",
        "üîπ **mAP@0.5:0.95** means we compute the **average precision** at **different IoU thresholds** from **0.5 to 0.95**, increasing in steps of **0.05**.\n",
        "\n",
        "- **IoU ‚â• 0.5** ‚Üí Loose match  \n",
        "- **IoU ‚â• 0.75** ‚Üí Stricter match  \n",
        "- **IoU ‚â• 0.95** ‚Üí Extremely strict match  \n",
        "\n",
        "**mAP@0.5:0.95** takes the **average of all these values**, giving us a single number that represents how well the model performs **across different difficulty levels**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd2ac986ab9db3c",
      "metadata": {
        "id": "5cd2ac986ab9db3c"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install torchmetrics\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81209ee3",
      "metadata": {
        "id": "81209ee3"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "# Initialize metric\n",
        "metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
        "\n",
        "def validate(model, dataloader, device):\n",
        "    \"\"\"Evaluates the model using mAP@0.5:0.95.\"\"\"\n",
        "    model.eval()\n",
        "    metric.reset()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in tqdm(dataloader):\n",
        "            images = [img.to(device) for img in images]\n",
        "            preds = model(images)\n",
        "\n",
        "            # Convert predictions to correct format\n",
        "            processed_preds = []\n",
        "            for pred in preds:\n",
        "                processed_preds.append({\n",
        "                    \"boxes\": pred[\"boxes\"].cpu(),\n",
        "                    \"scores\": pred[\"scores\"].cpu(),\n",
        "                    \"labels\": pred[\"labels\"].cpu()\n",
        "                })\n",
        "\n",
        "            # Convert ground truth targets\n",
        "            processed_targets = []\n",
        "            for target in targets:\n",
        "                gt_boxes = []\n",
        "                gt_labels = []\n",
        "                for obj in target['annotation']['object']:\n",
        "                    label = obj['name']\n",
        "                    box = obj['bndbox']\n",
        "                    xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "                    gt_boxes.append([xmin, ymin, xmax, ymax])\n",
        "                    gt_labels.append(voc_classes[label])\n",
        "\n",
        "                processed_targets.append({\n",
        "                    \"boxes\": torch.tensor(gt_boxes).cpu(),\n",
        "                    \"labels\": torch.tensor(gt_labels).cpu()\n",
        "                })\n",
        "\n",
        "            # Update metric\n",
        "            metric.update(processed_preds, processed_targets)\n",
        "\n",
        "    return metric.compute()  # Compute final mAP scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0abbe7",
      "metadata": {
        "id": "bc0abbe7"
      },
      "source": [
        "## **4Ô∏è‚É£ Running Training & Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c395d8",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "35c395d8",
        "outputId": "0525c9f8-ee01-46dd-a031-974a419c46bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2501/2501 [46:17<00:00,  1.11s/it]\n",
            " 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 2220/4952 [43:44<49:41,  1.09s/it]"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "num_epochs = 10  # Set number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "    mAP_results = validate(model, test_loader, device)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
        "    print(f\"mAP@0.5:0.95 for Test: {mAP_results['map']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0a7d245",
      "metadata": {
        "id": "d0a7d245"
      },
      "source": [
        "## **5Ô∏è‚É£ Visualizing Predictions vs. Ground Truth**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qetjqo8aClQA",
      "metadata": {
        "id": "qetjqo8aClQA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torchvision.transforms.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1447c4",
      "metadata": {
        "id": "7e1447c4"
      },
      "outputs": [],
      "source": [
        "# Select 3 random test images\n",
        "test_indices = [2777, 4742, 777]\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Create figure with 5√ó2 subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(30, 60))\n",
        "axes = axes.ravel()  # Flatten axes for easy iteration\n",
        "\n",
        "for i, idx in enumerate(test_indices):\n",
        "    test_img, test_target = test_dataset[idx]\n",
        "\n",
        "    # Extract Ground Truth Boxes & Labels\n",
        "    gt_boxes = []\n",
        "    gt_annotations = []\n",
        "    for obj in test_target['annotation']['object']:\n",
        "        box = obj['bndbox']\n",
        "        xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
        "        gt_boxes.append([xmin, ymin, xmax, ymax])\n",
        "        gt_annotations.append(obj['name'])\n",
        "\n",
        "    # Run Model on Test Image\n",
        "    with torch.no_grad():\n",
        "        pred = model([test_img.to(device)])\n",
        "\n",
        "    pred = pred[0]\n",
        "\n",
        "    # Extract Predictions\n",
        "    pred_boxes = pred['boxes'].cpu()\n",
        "    pred_annotations = pred['labels'].cpu()\n",
        "    pred_scores = pred['scores'].cpu()\n",
        "\n",
        "    # Apply Confidence Threshold (Only keep predictions with score ‚â• 0.8)\n",
        "    valid_mask = pred_scores >= 0.8\n",
        "    pred_annotations = pred_annotations[valid_mask]\n",
        "    pred_boxes = pred_boxes[valid_mask]\n",
        "\n",
        "    # Convert Predicted Labels from Numeric to Class Names\n",
        "    pred_annotations = [reverse_voc_classes[val.item()] for val in pred_annotations]\n",
        "\n",
        "    # Overlay GT & Predictions on Image\n",
        "    img = F.to_pil_image(test_img)\n",
        "    ax = axes[i]\n",
        "    ax.imshow(img)\n",
        "\n",
        "    # Plot Ground Truth in RED\n",
        "    for bbox, annotation in zip(gt_boxes, gt_annotations):\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x_min, y_min - 5, annotation, color='r', fontsize=24, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "    # Plot Predictions in GREEN\n",
        "    for bbox, annotation in zip(pred_boxes, pred_annotations):\n",
        "        x_min, y_min, x_max, y_max = bbox\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "\n",
        "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x_min, y_min - 5, annotation, color='g', fontsize=24, bbox=dict(facecolor='white', alpha=0.7))\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Image {idx}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0f36d4",
      "metadata": {
        "id": "bd0f36d4"
      },
      "source": [
        "### Contributed by: Mohamed Eltayeb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4d695b2",
      "metadata": {
        "id": "d4d695b2"
      },
      "source": [
        "![image.png](https://i.imgur.com/a3uAqnb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a12c26",
      "metadata": {
        "id": "a2a12c26"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2046.963277,
      "end_time": "2025-02-02T15:48:34.514608",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-02-02T15:14:27.551331",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}